{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(False) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"./\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 5\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 128\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 1\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 5\n",
    "\n",
    "# Size of feature maps in generat\n",
    "ngf = 256\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 256\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 250\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39mtranspose(vutils\u001b[38;5;241m.\u001b[39mmake_grid(\u001b[43mreal_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m1\u001b[39m], padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu(),(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.Grayscale(num_output_channels=1),\n",
    "                               transforms.CenterCrop(image_size),  # Center crop to 512x512\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5), (0.5)),\n",
    "                           ]))\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:1\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:1], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*16) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 64 x 64\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 128 x 128\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "#  to ``mean=0``, ``stdev=0.02``.\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            nn.utils.spectral_norm(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False)),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.utils.spectral_norm(nn.Conv2d(ndf * 16, 1, 4, 1, 0, bias=False)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "# Handle multi-GPU if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Apply the ``weights_init`` function to randomly initialize all weights\n",
    "# like this: ``to mean=0, stdev=0.2``.\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "img_list_only = []\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "        output = netD(real_cpu).view(-1)\n",
    "\n",
    "        errD_real = criterion(output, label)\n",
    "\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "\n",
    "        errD_fake = criterion(output, label)\n",
    "\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "\n",
    "        optimizerD.step()\n",
    "\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "\n",
    "        output = netD(fake).view(-1)\n",
    "\n",
    "        errG = criterion(output, label)\n",
    "\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                    errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            img_list_only.append(fake)\n",
    "\n",
    "            # for j in range(len(fake)-1):\n",
    "            #     save_image(fake[j], out + str(epoch) + '_' + str(j) + '_img.png')\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
